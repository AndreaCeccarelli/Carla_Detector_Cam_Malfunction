\documentclass[14pt]{extarticle}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[english]{babel}
\usepackage[final]{pdfpages}
\usepackage{imakeidx}
\usepackage{version}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{color}
\usepackage{multicol}
\usepackage{caption}
\usepackage{subcaption}


\definecolor{javared}{rgb}{0.6,0,0} % for strings
\definecolor{javagreen}{rgb}{0.25,0.5,0.35} % comments
\definecolor{javapurple}{rgb}{0.5,0,0.35} % keywords
\definecolor{javadocblue}{rgb}{0.25,0.35,0.75} % javadoc
 
\lstset{language=Python,
    keywordstyle=\color{ja\tableofcontentsvapurple},
    basicstyle=\small,
    commentstyle=\color{javagreen},
    stringstyle=\color{javadocblue},
    showstringspaces=false,
    breaklines=true,
    frameround=ffff,
    frame=single,
    rulecolor=\color{black}
} 
\makeindex[columns=3, title=Alphabetical Index, intoc]
\usepackage[backend=bibtex]{biblatex}
\addbibresource{bibliografia.bib}


\begin{document}

\title{\includegraphics[scale=0.2]{download.jpeg} \vspace{2cm} Scuola di Scienze  Matematiche,Fisiche e Naturali \\ Corso di Laurea in Informatica \\Tesi di Laurea \\ \texttt{Individuazione di immagine alterate da una telecamera RGB mal funzionante attraverso un agente addestrato}\\ \texttt{Detection of failed images from an RGB camera through trained agents}}
\author{\texttt{Pietro Bernabei} \\ \texttt{Anno Accademico 2019/20}}
\date{}
\maketitle
\newpage
\tableofcontents

\newpage
\section{Introduzione}
\subsection{Motivazioni}
Nel giro di un paio di secoli il mondo,si  è passati dal viaggiare in groppa a un cavallo, alla groppa di una macchina, dal essere il conducente, al condotto. Le macchine a guida autonoma parziali e totali stanno diventando ogni giorno che avanza, realtà. Questa rivoluzione sta permeando il nostro stile di vita, diventando sempre di più accessibili.  Questa presenza sempre più forte nella quotidianità di tutti i giorni, fa si che queste tecnologia debba rispettare degli standard sempre più stringenti, garantendo la continuità del servizio, l'assenza di possibili errori e malfunzionamento.
Come per un essere umano, che soffre di miopia, guidare senza occhiali è pericoloso, anche per il sistema di guida autonoma, guidare con le telecamere affette da guasti porta a incidenti.
Come verrà espresso in seguito, le telecamere negli attuali sistemi, ricoprono un ruolo decisivo nel momento decisionale della guida autonoma, e un loro malfunzionamento nel processo di acquisizione ripercuote nel sistema decisionale, errori non molto graditi al guidatore.

\subsection{Obiettivo}
Nella seguente tesi si propone un detector, ovvero un sistema software in grado di rilevare, nel flusso di immagini generate dalla telecamera di un mezzo a guida autonoma, la presenza di guasti nel sistema di acquisizione, come congelamento, pixel bruciati,e tanti altri, per poi notificare i possibili esiti al sistema  decisionale del mezzo, prevenendo lo sviluppo di fallimenti.
\subsection{Organizzazione del lavoro} 
Come verrà esposto in seguito un veicolo a guida autonoma, presenta una suite di sensori, attraverso i quali "vede" l'ambiente circostante. Su queste percezioni, il suo sistema decisionale guida il veicolo in sicurezza.
Un particolare componente di questa suite di sensori, sono le fotocamere RGB le quali risultano le più impiegate e importanti tra le tutte, e proprio la  loro importanza fa si che un loro guasto,  comporti un fallimento nel sistema decisionale.
Per questo il detector è posto tra il  sistema di acquisizione della fotocamera RGB e il sistema decisionale, al fine di rilevare la presenza dei guasti nel sensore e comunicare questa al sistema di guida su cui prenderà  le sue decisioni.
Il detector descritto nella seguente tesi impiega una particolare forma di intelligenza artificiale, rete neurale convoluzionale.
Come verrà esposto meglio in seguito, una convolutional neural network è una particolare forma di supervised learning, particolarmente efficiente in ambienti grafici, sopratutto per la classificazione di  immagini o identificazione di elementi al loro interno. Gli agenti supervisionati, e quindi anche le CNN richiedono una fase di training, nella quale viene definita  la loro conoscenza di base (Knowledge base), su cui prenderanno le loro decisioni.
Detto ciò lo sviluppo del detector si divide in:
\begin{itemize}
\item Definizione e generazione del dataset
\item Creazione del modello della ConvNet
\item Training del modello 
\item Testing del modello
\end{itemize}

\section{Fondamenti teorici}
\subsection{Sistemi critici}
Ogni giorno, una persona usa infrastrutture, mezzi, telecomunicazioni, e servizi di qualsiasi genere, affidandosi totalmente al loro funzionamento, alla loro continuità, dando per scontato che non possano subire guasti o malfunzionamenti, perché ove questi avvengano il risultato sarebbe devastante per tutto il nostro sistema di vita. Queste componenti si definiscono come \textbf{sistemi critici}, e  il loro corretto funzionamento, dipende dalla nostra capacità di analizzarne aspetti quantitativi relativi sia a caratteristiche prestazionali, quali velocità di elaborazione o altre misure di efficienza,
sia caratteristiche di sicurezza, disponibilità o affidabilità che dimostrino e ci
convincano della adeguatezza dei nostri manufatti per i compiti sempre più
critici e delicati per i quali li utilizziamo.

\subsubsection{Dependability}
La \textbf{dependability} è una delle proprietà fondamentali dei sistemi informatici insieme a funzionalità, usabilità, performance e costo. Per fornirne una prima definizione, è necessario illustrare i concetti di: \cite{avizienis2004basic}.
\begin{itemize}
\item \textbf{Servizio:} Il servizio fornito da un sistema è il comportamento del sistema stesso, così come viene percepito dai suoi utenti.
\item \textbf{Utente:}Un utente di un sistema è un altro sistema che interagisce attraverso l'interfaccia del servizio.
\item \textbf{Funzione di sistema:}La funzione di un sistema rappresenta che cosa ci attendiamo dal sistema; la descrizione della funzione di un sistema è fornita attraverso la sua specifica funzionale. Il servizio è detto corretto se realizza la funzione del sistema.
\end{itemize}

Quindi \textbf{dependability} è
la capacità di un sistema di fornire un servizio su cui è possibile fare affidamento in
modo giustificato.\cite{bondavalli2011analisi}
\begin{comment}
Una definizione alternativa, che stabilisce un criterio per decidere se un determinato servizio è dependable, definisce la \textbf{dependability} di un sistema
come la capacità di evitare fallimenti che siano più frequenti e più severi del
limite accettabile \cite{avizienis2004basic}.

Ciò di cui la definizione precedente non tiene conto, è che il comportamento del nostro sistema, sia sempre definito nella sua totalità, senza ambiguità e precisa. 
Per questo la dependability può essere vista come una misura di quanta fiducia possiamo riporre in modo giustificato sul servizio erogato dal sistema stesso.\cite{bondavalli2011analisi}
\end{comment}
%Un'esposizione sistematica dei concetti relativi alla dependability consiste di tre parti:
La dependability presenta tre diversi concetti di cui è necessario tenere conto: \cite{bondavalli2011analisi}
\begin{itemize}
\item \textbf{Le minacce o impedimenti alla dependability.} Gli impedimenti sono le
cause potenziali di comportamenti non previsti.
\item \textbf{Gli attributi della dependability.} Gli attributi ci permettono di esprimere
e verificare il livello di dependability richiesto od ottenuto.
\item \textbf{I mezzi per ottenere la dependability.} I mezzi sono le tecniche che
permettono di ottenere comportamenti corretti, nonostante il verificarsi
degli impedimenti.
\end{itemize}

\subsubsection{Le Minacce: guasti, errori e fallimenti}
Si definisce come:\cite{bondavalli2011analisi}
\begin{itemize}
\item \textbf{guasto:} la causa accertata o ipotizzata di un errore, derivante da malfunzionamenti di componenti, interferenze ambientali di natura fisica, sbagli dell'operatore o da una progettazione fallace.
\item \textbf{errore:} è la parte dello stato del sistema che può causare un susseguente fallimento; in alternativa si definisce errore la manifestazione di un guasto all'interno di un programma o di una struttura dati.
\item \textbf{fallimento:} di sistema è un evento che occorre quando un errore raggiunge l'interfaccia di servizio, alterando il servizio stesso. Quando un sistema viola la sua specifica di servizio si dice che è avvenuto un fallimento; il fallimento è quindi una transizione da un servizio corretto a un servizio non corretto. La transizione inversa, da un servizio non corretto ad uno corretto, è detta ripristino.
\end{itemize}

\subparagraph{Il guasto} può rimanere dormiente per un certo periodo, fino alla sua attivazione. L'attivazione di un guasto porta ad un errore, che è la parte dello
stato del sistema che può causare un successivo fallimento. 
I guasti di un sistema possono essere classificati secondo diversi punti di vista, ad esempio
fisico, logico e di interazione. Un'altra suddivisione può essere fatta in base
alla natura del guasto: un guasto può essere intenzionale o accidentale, malizioso oppure non malizioso; ed ancora in base alla persistenza dove abbiamo
guasti permanenti, transienti ed intermittenti. Per una tassonomia completa
si rimanda a \cite{avizienis2004basic} 

\subparagraph{Il fallimento} di un componente si verifica quando il servizio fornito devia
dalla sua specifica: si verifica nel momento in cui un errore del componente
si manifesta alla sua interfaccia, e diventa quindi un guasto per il sistema. Il
fallimento è quindi l'effetto, osservabile esternamente, di un errore nel sistema; gli errori sono in stato latente fino a che non vengono rilevati e/o non
producono un fallimento.

La deviazione dal servizio corretto può assumere diverse forme, che vengono chiamate modi di fallimento e sono classificati secondo la loro
gravità (severity).

\begin{comment} I modi di fallimento caratterizzano un servizio non corretto
da quattro punti di vista:
\begin{itemize}
\item Il dominio dei fallimenti,
\item La possibilità di rilevare i fallimenti,
\item La consistenza dei fallimenti,
\item Le conseguenze dei fallimenti.
\end{itemize}

Una completa analisi dei rischi (risk analysis, \cite{cenelec199950126}) e delle modalità di fallimento ad essi associate è necessaria per lo sviluppo di sistemi critici; tali
attività sono generalmente classificate come obbligatorie negli stessi standard
per la certificazione del rispetto di requisiti di dependability (ad esempio in
\cite{cenelec199950126}).
\end{comment}
\subparagraph{Chain Fault-Error-Failure}
Un sistema è formato da un insieme di componenti che interagiscono tra
loro, perciò lo stato del sistema è l'insieme degli stati dei suoi componenti. Un
guasto causa inizialmente un errore nello stato di uno (o più) componenti, ma
il fallimento del sistema non si verifica fino a quanto l'errore non raggiunge l'interfaccia del servizio. La propagazione di errori può permettere ad un errore
di raggiungere l'interfaccia di servizio. Questo insieme di meccanismi costituisce la catena di impedimenti \textbf{guasto-errore-fallimento} (fault-error-failure)
La propagazione all'interno di un componente (propagazione interna) è
causata dal processo di elaborazione: un errore viene successivamente trasformato in altri errori.
%guarda se aggiungere anche il parallelismo con carla forse è utile
\begin{figure}
\centering
\includegraphics[scale=0.5]{catena_gef.png}
\caption{Catena di guasto errore fallimento (figura ripresa da \cite{avizienis2004basic})}
\label{fig:gef}
\end{figure}
 La propagazione da un componente A verso un componente B che riceve un servizio da A (propagazione esterna) (Fig:\ref{fig:gef}) avviene
quando un errore raggiunge l'interfaccia di servizio del componente A. A questo punto, il servizio che B riceve da A diventa non corretto e il fallimento di
A appare a B come un guasto esterno, e si propaga come un errore all’interno
di B (Fig:\ref{fig:gef}). 
Per esempio, nel caso di un veicolo a guida autonoma, un guasto nella lente o nel sensore della fotocamera,  genera un errore. Quando raggiunge l'interfaccia di servizio  genera un fallimento per la fotocamera. Per il sistema di visione artificiale, il fallimento della fotocamera risulta come un guasto, siccome non eroga il servizio correttamente. Questo genera un errore interno al sistema, che come viene dimostrato nella seguente tesi \cite{secci2020failures}, porta a un fallimento sistema decisionale della macchina a guida autonoma. Il detector proposto si pone tra la fotocamera e il sistema decisionale per intercettare il fallimento della fotocamera impedendo cosi il fallimento del sistema decisionale che avrebbe conseguenze catastrofiche sui passeggeri e su tutto ciò che li circonda.
\subsubsection{Gli attributi della dependability}
Il concetto di dependability è la sintesi di più attributi che forniscono misure
quantitative o qualitative del sistema:
\begin{itemize}
\item \textbf{Affidabilità (reliability):} è la capacità del sistema di erogare un servizio corretto in modo continuo; misura la fornitura continua di un servizio
corretto.
\item \textbf{Manutenibilità (maintainability):} è la capacità del sistema di subire modifiche e riparazioni; misura il tempo necessario per ristabilire un servizio
corretto.
\item \textbf{Disponibilità (availability):} è la prontezza del sistema nell'erogare un
servizio corretto; misura la fornitura di servizio corretto, rispetto all'alternanza fra servizio corretto e non corretto.
\item \textbf{Confidenzialità (confidentiality):} è l'assenza di diffusione non autorizzata di informazioni; misura l'assenza di esposizione non autorizzata di
informazione.
\item \textbf{Integrità (integrity):} descrive l'assenza di alterazioni improprie del sistema; misura l’assenza di alterazioni improprie dello stato del sistema.
\item \textbf{La sicurezza (safety)} è l'assenza di conseguenze catastrofiche sugli
utenti e sull'ambiente circostante. \begin{comment} 
La safety può essere vista come l'affidabilità
del sistema.
, considerando come corretti anche gli stati in cui il sistema subisce
un fallimento benigno (possiamo quindi fondere in un unico stato sia gli stati
corretti che i fallimenti benigni del sistema, e valutare in questo nuovo schema
l'affidabilità)\end{comment}.
\item \textbf{sicurezza (security)} è vista come la contemporanea esistenza di availability solo per gli utenti autorizzati, confidentiality, e
integrity, dove per “improprie” si intende “non autorizzate” \cite{nicol2004model}.
\end{itemize}
%Ciascuno di questi attributi può essere più o meno importante in base all'applicazione: la disponibilità del servizio è sempre richiesta, anche se può
%variare sia l'importanza relativa che il livello quantitativo richiesto mentre, gli altri attributi possono essere richiesti
%o meno.
Nella loro definizione, la disponibilità e la affidabilità evidenziano la
capacità di evitare i fallimenti, mentre la safety e la security evidenziano la
capacità di evitare specifiche classi di fallimenti come ad esempio fallimenti
catastrofici e accesso non autorizzato alle informazioni.
%I requisiti di dependability di un sistema sono forniti attraverso una descrizione degli obiettivi richiesti per uno o più degli attributi sopra descritti,
%rispetto alle modalità di fallimento previste per il sistema.
Se i modi di fallimento previsti sono specificati e limitati, si parla di sistemi \textbf{fail-controlled}; un
sistema i cui fallimenti sono limitati soltanto all'interruzione del servizio sono
chiamati \textbf{fail-stop} o \textbf{fail-silent}. I sistemi \textbf{fail-safe}, invece, sono quelli per cui i
fallimenti possibili sono solamente fallimenti non catastrofici.
\begin{comment}
Il grado con cui un sistema possiede questi attributi deve essere interpretato in senso probabilistico e non in senso assoluto, deterministico: a causa
dell'inevitabile occorrenza dei guasti i sistemi non sono mai totalmente disponibili, affidabili, safe o secure. Per questo gli attributi di dependability
possono essere definiti in senso probabilistico cosi da poterli trattare in modo
quantitativo.
Ad esempio:
\begin{itemize}
\item L'affidabilità può essere rappresentata dalla probabilità che il sistema
non fallisca durante il periodo di missione del sistema. Se si assumono
distribuzioni esponenziali, possiamo rappresentare l'affidabilità tramite
il tasso di fallimenti (ad esempio, in numero medio di fallimenti all’ora).
\item La disponibilità è la probabilità che il sistema sia operativo al tempo t,
considerando l'alternanza fra gli stati di servizio corretto e servizio non
corretto.
\item La manutenibilità può essere rappresentata dalla velocità con cui viene
ripristinato un servizio corretto dopo un fallimento.

\end{itemize}
**guarda se aggiungere ultima parte con le formule
\end{comment}
\subsubsection{I mezzi per ottenere la dependability}
Lo sviluppo di sistemi dependable richiede l'utilizzo combinato di quattro
tipologie di tecniche:
\begin{itemize}
\item \textbf{prevenzione dei guasti}, per prevenire l'occorrenza o introduzione di
guasti nel sistema;
\item \textbf{tolleranza ai guasti}, per erogare un servizio corretto anche in presenza
di guasti;
\item \textbf{rimozione dei guasti}, per ridurre il numero o la gravità dei guasti;
\item \textbf{previsione dei guasti}, per stimare il numero di guasti presenti nel
sistema, la loro incidenza futura, o le loro probabili conseguenze.
\end{itemize}
\subparagraph{Prevenzione dei guasti: }
La “Fault Prevention” viene effettuata ricorrendo a tecniche e processi di controllo di qualità sia durante la progettazione del software che durante la produzione dei componenti hardware.
\begin{comment} 
Queste tecniche comprendono ad esempio
la programmazione strutturata e modulare, l'uso di linguaggi fortemente tipati, di editori guidati dalla sintassi e compilatori certificati per quanto riguarda
il software, mentre per quanto riguarda l’hardware l’uso di rigorosi processi
produttivi e di strumenti per la progettazione come linguaggi di alto livello
(VHDL). Guasti di origine fisica vengono prevenuti tramite specifiche protezioni, ad esempio dalle radiazioni e interferenze elettromagnetiche. Guasti
che originano da interazioni umane possono invece essere prevenuti tramite
un'appropriata formazione del personale, o la creazione di procedure di manutenzione rigorose. Guasti originati da attacchi esterni possono essere prevenuti
tramite firewall e dispositivi di sicurezza simili.\end{comment}
\subparagraph{Tolleranza ai guasti: }
La “Fault Tolerance” mira a preservare l'erogazione di un servizio corretto in
presenza di guasti attivi. Essa viene solitamente implementata tramite rilevazione di errori (error detection) e conseguente recupero dello stato del sistema
(system recovery). In particolare, la rilevazione degli errori origina un segnale
di errore all'interno del sistema; esistono due classi di tecniche di rilevazione
di errori: \textbf{concurrent error detection} viene effettuata durante l'erogazione del
servizio, \textbf{preemptive error detection} viene effettuata quando l'erogazione del
servizio è sospesa e controlla la presenza di errori latenti e guasti dormienti.
Il recupero dello stato del sistema trasforma uno stato che contiene uno o più
errori attivi (ed eventualmente guasti), in uno stato che non contiene errori
rilevati e guasti che possono essere nuovamente attivati. Il recovery consiste
in \textbf{error handling} e \textbf{fault handling}. \textbf{Error handling} elimina gli errori dallo
stato del sistema \begin{comment} 
e può assumere tre forme: rollback, dove la trasformazione consiste nel ritornare ad uno stato in cui si trovava il sistema prima della
rilevazione dell'errore; rollforward, dove si porta il sistema in uno stato del
tutto nuovo, e compensation, dove lo stato contiene abbastanza ridondanza
per eliminare la parte erronea.\end{comment}
 mentre \textbf{Fault handling} impedisce che i guasti che sono
stati localizzati vengano nuovamente attivati. \begin{comment}
, attraverso quattro fasi:
\begin{itemize}
\item \textbf{fault diagnosis} identifica l'origine della cause degli errori, in termini di
locazione e tipo;
\item \textbf{fault isolation} isola logicamente o fisicamente il componente, impedendogli di partecipare all'erogazione del servizio, trasformando il guasto in
un guasto dormiente;
\item \textbf{system reconfiguration} che riconfigura il sistema, ad esempio attivando componenti di riserva o ridistribuendo il carico tra i componenti
funzionanti;
\item \textbf{system reinitialization}, che esegue i controlli e gli aggiornamenti necessari in seguito alla nuova configurazione.

\end{itemize}
Solitamente l'attività di fault handling è seguita da azioni di manutenzione correttiva, che rimuovono i guasti isolati dal fault handling, ad esempio
sostituendo un componente segnalato come guasto. Il fattore che distingue la
fault tolerance dalla manutenzione (maintenance) è che quest'ultima richiede
l'intervento di un agente esterno.\end{comment}

\subparagraph{Rimozione dei guasti: }
La \textbf{Fault Removal} viene effettuata sia durante la fase di sviluppo, che durante la vita operazionale del sistema.
La rimozione dei guasti è uno degli obiettivi del processo di verifica e validazione (V\&V).
La verifica è un processo attraverso cui si determina se il sistema soddisfa alcune proprietà determinate dalle
specifiche o imposte all'inizio della fase di sviluppo; se cosi non è si cerca di
individuare il guasto che impedisce di soddisfare tali proprietà e lo si corregge.
La validazione consiste invece nel controllare se il sistema soddisfa le proprie
specifiche e se le specifiche descrivono adeguatamente la funzione intesa per il
sistema.
% Le tecniche di verifica possono essere classificate in base alla necessità di esercitare il sistema. La verifica di un sistema senza la sua esecuzione
%è una verifica statica, altrimenti è una verifica dinamica.
La rimozione dei 
guasti durante la sua vita operazionale è manutenzione correttiva o preventiva. La manutenzione correttiva ha l'obiettivo di rimuovere guasti che sono
stati segnalati come la causa di uno o più errori, la manutenzione preventiva
cerca di scovare e rimuovere i guasti prima che causino degli errori durante la
normale operazione del sistema.
\subparagraph{Previsione dei guasti: }
La \textbf{Fault Forecasting} è condotta effettuando una valutazione del comportamento del sistema rispetto all'occorrenza e attivazione dei guasti. La valutazione può essere di due tipi: qualitativa, che mira ad identificare, classificare
e valutare i modi di fallimento o le combinazioni di eventi che porterebbero ad
un fallimento del sistema; quantitativa (o probabilistica), che mira a valutare
in termini probabilistici il grado con cui alcuni attributi vengono soddisfatti
dal sistema; questi attributi sono in questo caso visti come misure.\begin{comment} 
Alcuni
metodi di analisi sono specifici per una valutazione qualitativa o quantitativa, mentre altri possono essere utilizzati per entrambi i tipi di analisi. I due approcci principali per il fault forecasting di tipo probabilistico sono la modellizzazione e il testing. Questi approcci sono complementari: la costruzione di
un modello del sistema richiede delle informazioni su alcuni processi di base
del sistema, che possono essere acquisite tramite testing. Generalmente, un
sistema eroga diversi servizi, e spesso esistono due o più modi di erogazione
del servizio, ad esempio da servizio a pieno regime, a servizio di emergenza.
Questi modi distinguono la qualità o completezza del servizio erogato. Misure di dependability collegate alla qualità del servizio erogato (performance)
vengono solitamente riassunte nella nozione di performability \cite{smith1988performability}.
I due principali approcci alla previsione dei guasti quantitativa sono la
costruzione di modelli e la loro soluzione (analitica o tramite simulazione), in
cui il comportamento del sistema è riprodotto tramite un modello (tipicamente
un modello stato-transizione), e la osservazione e la valutazione (anche tramite
test specifici) sperimentale. Le attività di fault injection sono un esempio di
valutazione sperimentale del sistema ai fini della fault forecasting, in quanto
permette di esaminare l'evoluzione e le conseguenze dei guasti in un sistema.
\end{comment}

\subsection{Artificial Intelligence}
Nel pensiero comune quando si pensa all'intelligenza artificiale, si immagina un entità in grado di pensare, in grado di sostituirci. Nella realtà, l'intelligenza artificiale è quella branca dell'Informatica che dato un determinato problema, definisce  agenti in grado di trovare in modo efficace le migliori soluzioni. Quindi il campo di  applicazione dell'intelligenza artificiale non è unico ma è suddiviso in sottodiscipline, dove si va da aree più generali come l'apprendimento e la percezione, ad altre più specializzate come il gioco degli scacchi e la dimostrazioni di teoremi matematici, ad altre anche più complesse e complete come la guida autonoma.

\subsubsection{Agente razionale,Misura di prestazione,Ambiente}
Nello specifico l'intelligenza artificiale si occupa di progettare \textbf{agenti razionali}, che posti in un \textbf{ambiente}, riescano a massimizzare la propria \textbf{misura di prestazione}.\cite{russell2005intelligenza}
Un \textbf{agente} è definito  come un sistema che percepisce il suo ambiente attraverso dei sensori e agisce su di esso mediante attuatori. Per farsi un idea, si può assumere che l'essere umano sia un agente, che fornito di sensori, come occhi,orecchie e altri organi, sente l'ambiente circostante, e attraverso altrettanti organi (attuatori), come mani,gambe e bocca, interagisce con l'ambiente o altri agenti.
Un agente fisico, come una macchina a guida autonoma, ha sensori come: telecamere, sensori a infrarossi,e radar, attraverso cui vedere, mentre  per agire: motore,freni, e tanti altri. 
Un altra cosa che accomuna tutti gli agenti, sono le  \textbf{percezioni}, termine usato per indicare gli input percettivi dell'agente in dato istante.\cite{russell2005intelligenza}
La totalità delle percezioni generate dall'agente in tutta la sua storia, è definita invece come \textbf{sequenza percettiva}, dove in generale la scelta dell'azione di un agente in un qualsiasi istante può dipendere dall'intera sequenza percettiva osservata fino a quel momento.
In termini matematici la rappresentazione del comportamento di un agente, è descritto dalla \textbf{funzione agente}, che descrive la corrispondenza tra una qualsiasi sequenza percettiva e una specifica azione.
Se la funzione agente è la rappresentazione matematica dell'agente, con il termine \textbf{programma agente} si indica la sua implementazione concreta  in  esecuzione sull'architettura dell'agente.  \cite{russell2005intelligenza}
\begin{figure}
\centering
\includegraphics[scale=0.2]{AI_Agente_2.png}
\caption{Esempio di agente razionale}
\label{fig:agra}
\end{figure}
\subsubsection{Agenti che apprendono}
Si definisce un agente che apprende se migliora le proprie prestazioni nelle attività future dopo aver effettuato le osservazioni
sul mondo. \cite{russell2005intelligenza}
Questo processo è decisivo per un agente per tre motivi:
\begin{itemize}
\item il programmatore non può anticipare tutte le possibili situazioni che l'agente si potrà trovare davanti.
\item il programmatore non può anticipare tutti i cambiamenti nel tempo.
\item il programmatori spesso non sanno come programmare loro la soluzione a un problema.
\end{itemize}\cite{russell2005intelligenza}

Detto ciò, come esistono agenti diversi per problemi diversi, esistono apprendimenti diversi per ogni agente, che possono essere applicati alle varie componenti che  lo compongono.
Una loro classificazione si basa su quattro fattori: la componente che viene migliorata, la rappresentazione usata per i dati e i componenti, la conoscenza a priori, e per ultimo i feedback disponibili da apprendere. Su quest'ultimo sono stati sviluppati tre diversi paradigmi di apprendimento:
\begin{itemize}
\item \textbf{unsupervised learning:} l'agente apprende patterns dall'input senza feedback espliciti. 
\item \textbf{reinforcement learning:} l'agente apprende da una serie di rinforzi, punizioni o premi.
\item \textbf{supervised learning:} all'agente sono forniti coppie di valori input-output, da cui apprende una funzione che mappi il collegamento tra input e output. 
\end{itemize}
Nello specifico, il compito di un apprendimento supervisionato è quello che dato un training set di N esempi della forma input-output:
\begin{center}
$(x_{1}.y_{1}),(x_{2},y_{2}),...(x_{n},y_{n})$
\end{center}
dove ogni valore y è generato da un funzione sconosciuta $y=f(x)$, questa scopre una funzione $h$ che approssima la funzione $f$.  
Il learning problem si può definire in due maniere a seconda dell'output y: nel caso in cui  y è uno di  un insieme finito di valori il problema è definito  \textbf{classificazione} (binaria o booleana, nel caso di due sole scelte),mentre se y è un numero 
 è definito come \textbf{regressione}.
 
\subsubsection{Neural Network}
Una particolare forma di supervised learning, come ci suggerisce il titolo del seguite capitolo, sono le artificial neural network o più brevemente neural network. 
La \textbf{rete neurale} è composta da un insieme di nodi o unità, connesse da link orientati.\cite{russell2005intelligenza}
 La topologia e le proprietà dei nodi determinano le proprietà della rete.
\paragraph{Neurone}
Ciascun neurone o nodo, ha un insieme di input e produce un singolo output che può essere inviato a un gruppo di altri neuroni. Questo si attiva quando una combinazione lineare dei suoi input superano la sua threshold. L'output dei neuroni finali sarà il risultato dello scopo della rete.
\subparagraph{Funzione di attivazione}
In generale un neurone, calcola una somma pesata dei suoi input:
\[in_j=\sum_{i=0}^n  w_{i,j}a_{i}\]
A questa è applicata la \textbf{funzione di attivazione} g, dalla quale si ottiene l'output:
\[a_j=g(in_j)=g \Bigl (\sum_{i=0}^n  w_{i,j}a_{i} \Bigr )\]
\begin{comment}
Nel caso in cui la funzione di attivazione è una hard threshold, l'unità è chiamato \textbf{perceptron}, nel caso in cui questa sia una funzione logica, si usa il termine \textbf{sigmoid perceptron}.\end{comment}
\paragraph{Collegamenti}
I neuroni sono interconnessi tra di  loro,  al fine di  propagare il risultato della funzione di attivazione di un nodo padre a un nodo figlio.
%Un link tra una unita i e un unità j serve a propagare l'attivazione $a_{i}$ da i a j.
Oltre a propagare il risultato tra neuroni, il collegamento ha anche un peso numerico $w_{i,j}$ associato, il quale determina la forza e il segno della connessione.

\paragraph{feed-forward network}
Definito il modello matematico di ogni neurone, questi si connettono insieme attraverso i collegamenti, a formare una rete.
Esistono due principali tipologie di network: 
La \textbf{feed-forward network}, che prevede connessioni solo in una direzione, formando un DAG, directed acyclic graph. Ogni nodo riceve gli input dai nodi superiori e invia gli output a nodi inferiori. Non sono previsti cicli. Questa tipologia di rete, rappresenta una funzione del corrente input, senza mantenere al suo interno, nessun tipo di stato se non i pesi dei collegamenti.
Il \textbf{recurrent network}, prevede che i suoi neuroni usino il proprio output come feedback per il proprio input, cosi che i livelli di attivazione della rete formino un sistema dinamico che raggiunge uno stato stabile o
esibiscono oscillazioni o addirittura comportamenti caotici. Questa caratteristica, permette alla rete di supportare una memoria a breve termine.
\begin{comment}
\subparagraph{perceptron network}:
Una forma semplice di feed-forward network, è il perceptron network o anche chiamato single-layer neural networl. Questo prevede che i suoi neuroni di input siano direttamente connessi a quelli di output. 
\end{comment}

\subsection{Autonomous Driving}
Nell'ultimo decennio, il settore dell'Automotive è stato in continua evoluzione. In particolar modo, le grandi aziende automobilistiche e molti gruppi di ricerca si sono mossi verso l'automazione della guida ed
il miglioramento delle infrastrutture già esistenti. I motivi di questo
interesse del settore verso la guida autonoma sono vari:
\begin{itemize}
\item riduzione del numero di incidenti, anche fino al 90%,
\item  riduzione dei consumi,
\item  riduzione delle emissioni di $CO_{2}$ ,
\item  riduzione di tutte le varie problematiche della sicurezza
stradale.
\end{itemize}

\subsubsection{Livelli di automazione della guida}
La SAE International, un ente di standardizzazione automobilistica,  nel 2014, pubblica un nuovo standard internazionale, il J3016 ("Levels of Driving Automation").
In questo sistema di classificazione, all'aumentare del livello di automazione, la responsabilità del conducente passa dalla guida alle attività di
supervisione.
Lo standard suddivide l'automazione in 6 livelli, dal livello 0, in cui è assente qualsiasi tipologia di automazione, al livello 5, dove il sistema guida in maniera completamente autonoma.
Per una esposizione più completa si rimanda alla figura \ref{fig:classificazioneSAE}

\begin{itemize}
\item Livelli 0:
Veicoli privi di qualsiasi sistema di automazione.
\item Livello 1: 
Veicoli  i quali mettono a disposizione dell'autista, almeno un sistema di assistenza, per esempio, l'assistenza alla frenata.
\item Livello 2:
Veicoli i quali combinano due o più sistemi avanzati di assistenza alla guida, come l'adaptive cruise control, Lane-Keeping assist, e l'Automatic Emergency Braking. Tutti questi rientrano sotto la dicitura ADAS (Advanced Driver-Assistance Systems). 
\item Livello 3:
Veicoli definiti a guida semi-autonoma, siccome sono in grado di  gestire situazioni variegate, anche al di fuori di un contesto autostradale. Nel senso che si prendono la responsabilità di gestire tratti del tragitto, richiedendo però alla fine di questi la ripresa del controllo da parte del conducente. Nel caso questo non avvenisse, arresteranno in sicurezza il  veicolo.
\item Livello 4
Veicoli definiti a guida autonoma, sono dotati di un set di tecnologie in grado di procedere per lunghi tragitti, superando una varietà di ostacoli, come i caselli autostradali.Tuttavia il loro funzionamento sarà limitato:
\begin{itemize}
\item ad una determinata area geografica,
\item condizioni meteo avverse,
\item da una velocità massima e così via.
\end{itemize}
\item Livello 5
Veicoli privi di qualsiasi limite e conducente, capaci di procedere in qualsiasi condizione e luogo.
\end{itemize}
\begin{figure}
\centering
\includegraphics[scale=0.5]{SAE_level_automation.png}
\caption{classificazione SAE}
\label{fig:classificazioneSAE}
\end{figure}
\begin{comment}
Al momento la serie di veicoli con il livello più alto, è l'Autopilot di Tesla, che effettivamente si può ritenere di livello 4, ma per motivi legali è categorizzato come livello 3, e la navetta Olly, classificata come livello 4.
\end{comment}
\subsubsection{Visione artificiale}
Come definito nella sezione precedente dedicata all'intelligenza artificiale, ogni agente prende in input una sensazione, la elabora, prende una decisione, e la attua con gli attuatori. Applicando questo concetto all'autonomus driving, anche questi hanno una serie di sensori, che gli permettono di prendere coscienza dell'ambiente circostante in cui è immersa la vettura.
Le principali tecnologie che compongono la visione artificiale avanzata dei veicoli a guida autonoma sono\cite{das2018risk}:
\begin{itemize}
\item Fotocamere
\item Radar
\item Lidar
\item GPS
\end{itemize}
 come si può vedere in \ref{fig:maga}
Tutte insieme, forniscono un modello del ambiente circostante (3D),  attraverso immagini bidimensionali(2D).
Lo scopo della visione artificiale è proprio quella di riprodurre la vista umana.\cite{wiki:visart}
Le fotocamere fra tutte le tecnologie risultano essere la più presente e sfruttata al momento. Per questo motivo per il lavoro di tesi è stato sviluppato l'agente addestrato con questo scopo
\begin{figure}
\centering
\includegraphics[scale=0.8]{maga.jpeg}
\caption{Schema di posizionamento dei sensori su veicolo a guida autonoma}
\label{fig:maga}
\end{figure}

\subparagraph{LIDAR - Light Detection And Ranging}
Il Lidar è  solito essere installato sul tettuccio del veicolo, è un sensore che attraverso l'emissione e ricezione di segnali luminosi, laser, ottiene informazioni fisiche  dell'ambiente e  cinematiche sul  veicolo.
\begin{comment}

È una tecnologia di telerilevamento. I suoi usi sono molto vari soprattutto
perché il suo obiettivo principale è quello di raccogliere informazioni
3D e usare la luce sotto forma di un laser pulsato per misurare diverse
distanze. Il suo ruolo è quello di raccogliere informazioni cinematiche sul
veicolo e informazioni fisiche sull’ambiente circostante. Il sensore ottico
LIDAR è installato sul tettuccio del veicolo autonomo. È composto da un
laser, un filtro per obiettivo, un ricevitore, un regolatore di potenza, uno
specchio rotante e un processore integrato \citep{das2018risk}.
Diversamente dal Radar, che al posto della luce utilizza onde radio,3.2 visione artificiale
la distanza dell’oggetto è determinata misurando il tempo trascorso fra
l’emissione dell’impulso e la ricezione del segnale retrodiffuso. L’unità
di elaborazione deve essere collocata in una posizione abbastanza rialzata da terra; inoltre, sono necessarie misure di sicurezza per proteggere
l’unità da urti o vibrazioni derivanti da incidenti o dalla navigazione su
terreni accidentati, che potrebbero causare guasti al sistema.
\end{comment}
\subparagraph{RADAR - RAdio Detection And Ranging}
E' un sistema che utilizza onde radio trasmesse nell'ambiente, al fine di raccogliere informazioni sugli  ostacoli intorno al veicolo e aumentare la consapevolezza del  posizionamento degli altri veicoli presenti. Questo sensore tiene
d’occhio le altre auto e indica al veicolo autonomo di accelerare o rallentare
a seconda del comportamento degli altri conducenti \cite{das2018risk}.

\begin{comment}
È un sistema che utilizza onde radio per il rilevamento e la determinazione della posizione ed eventualmente della velocità di oggetti fissi o
mobili, come aerei, navi, veicoli, formazioni atmosferiche o il suolo. Le
onde radio vengono trasmesse nell'ambiente per raccogliere informazioni
sugli ostacoli intorno al veicolo e aumentare la consapevolezza del posizionamento degli altri soggetti davanti e dietro. Questo sensore tiene
d'occhio le altre auto e indica all'auto autonoma di accelerare o rallentare
a seconda del comportamento degli altri conducenti \cite{das2018risk}.
È utile anche per posteggiare il veicolo ed in questo caso si sente parlare
di sensori anteriori e posteriori.
\end{comment}

\subparagraph{Videocamera}
Le telecamere sono necessarie nel sistema di trasporto intelligente affinché si riconoscano gli ostacoli rispetto alla posizione e alla velocità del
veicolo in considerazione. Le immagini bidimensionali derivanti da una
singola fotocamera o le mappe 3D risultanti dall'utilizzo della doppia
fotocamera, potrebbero individuare in modo stereoscopico lo spazio disponibile per i movimenti autonomi del veicolo. Queste immagini o mappe
vengono utilizzate per estrarre informazioni quantitative dalle scene e
per tracciare gli obiettivi del veicolo. Le immagini sono segmentate in un
certo numero di pixel. Ogni pixel viene elaborato e memorizzato, il che richiede un'elevata velocità di calcolo ed un elevato spazio di archiviazione
\cite{das2018risk}.

\subparagraph{GPS - Global Positioning System}
Il Sistema di Posizionamento Globale, attraverso una rete dedicata di satelliti artificiali in orbita, fornisce a un terminale mobile o ad un ricevitore
GPS informazioni sulle sue coordinate geografiche e sul suo orario in
quasi tutte le condizioni atmosferiche ed ovunque non vi siano ostacoli
che potrebbero impedirne l'invio e la ricezione dei segnali (edifici molto
alti, gallerie, ecc.), generando errori dell'ordine di metri.

	
\subsubsection{Reti neurali applicate all'autonomus driving}
\begin{figure}
	\centering
	\includegraphics[scale=0.8]{adversarial_img.png}
	\caption{Adversial Examples}
	\label{fig:AdvExp}
	\end{figure}
Le Reti Neurali sono oggi ampiamente accettate come soluzioni dominanti per la visione artificiale, per il riconoscimento vocale e per l'elaborazione del linguaggio naturale. Nel campo della visione artificiale,le neural network elaborano le informazioni ottenute dai sensori, e ne estraggono informazioni utili al fine di prendere decisioni sul come muoversi, garantendo la sicurezza dei viaggiatori, e degli utenti esterni al veicolo. 
Questo sistema presenta comunque delle criticità, come una non corretta interpretazione dell'ambiente a causa di guasti, che minano la sicurezza dei passeggeri, ma anche tutto ciò che circonda il veicolo.%, come un caso di autopilot di Tesla.
Per quanto riguarda le prestazioni misurate su questa tecnologia, sono stati
registrati punteggi di accuratezza spesso corrispondenti a quelli di un
individuo umano, su benchmark chiave. Ben diversi, invece, sono sta-
ti i punteggi osservati quando si considerano  casi peggiori\cite{secci2020failures}. Con casi
peggiori si vuole intendere quei casi definiti Adversarial Examples \cite{engstrom2019exploring} o
input perturbati, anche solo leggermente, che dal punto di vista umano
risultano sempre comprensibili, mentre da quello della macchina, cambiano totalmente di significato. Un esempio di questo, è rappresentato dalla figura \ref{fig:AdvExp}. Questa mostra una rete neurale allenata nel riconoscimento di animali, che all'applicazione di un semplice filtro cambia totalmente risposta alla classificazione,mentre per un occhio umano questa variazione non modifica il significato dell'immagine.
Naturalmente, se queste reti vengono
utilizzate su veicoli che devono muoversi in un ambiente nel quale si
trova un'eterogeneità di soggetti e comportamenti, si parla di contesti
nei quali affidabilità, sicurezza e tutti gli altri attributi che definiscono la
dependability di un sistema sono al primo posto in ordine di importanza.

\subsubsection{Classificatori}
Una attività importante delle reti neurali nella guida autonoma, è il processo di classificazione; ovvero l'identificazione di particolari caratteristiche  intrinseche di un insieme di immagini o fotogrammi, sulle quali prendere delle decisioni.
Come detto in precedenza, le reti neurali presentano delle criticità in presenza di immagini prese in input trasformate spazialmente \cite{engstrom2019exploring}: con questo termine si intende rappresentare tutte le possibili trasformazioni
che un'immagine può subire, ad esempio ruotandola, tagliandola, scalandola ecc. Queste sono trasformazioni naturali e sono utilizzate in vari
contesti:
	\begin{itemize}
	\item verificare il livello di completezza di un classificatore,
	\item allenare una rete neurale,
	\item ricreare scenari o situazioni con cui il veicolo su cui è installata una
	determinata rete può avere a che fare.
	\end{itemize}

Con il termine "trasformata spazialmente" non ci si deve immaginare
una modifica profonda del fotogramma in questione, ma anzi, basta
anche solo una piccola rotazione, di meno di un grado, per far sì che il
classificatore confonda, ad esempio, un revolver con una trappola per topi
\cite{engstrom2019exploring}, il che, pensando ad un utilizzo anche diverso da quello che se ne fa
su un veicolo a guida autonoma, può far sorgere molte preoccupazioni e
domande.%(si pensi all’ambito aeroportuale).

\subsubsection{Object Recognition}
In generale, ci si riconduce al problema dell'Object Recognition o riconoscimento artificiale degli oggetti. Questo richiede che ci sia una nozione
di somiglianza visiva, difficile da far apprendere ad un sistema: significherebbe catturare e insegnare ad una macchina la nozione di percezione
umana \cite{engstrom2019exploring}.

Tuttavia, oggi, le DNN (Deep Neural Network) hanno raggiunto prestazioni all'avanguardia, talvolta competitive con la percezione umana.
Una della sfide principali nella guida autonoma è il cambiamento ambientale che un qualsiasi sistema autonomo può incontrare, ovvero: le
differenti distanze e angolazioni con cui viene percepito l'ambiente circostante dai sistemi di visione artificiale variano ogni volta che il veicolo si
muove nello spazio \cite{eykholt2018robust} .
Purtroppo, nel contesto della guida autonoma, anche un piccolo errore
di valutazione nella navigazione potrebbe causare incidenti altamente
devastanti.
	\begin{comment}
	Diversi gruppi di ricerca hanno sviluppato un modello preliminare di
	analisi del rischio per veicoli autonomi. In tali studi sono stati considerati
	dei sottosistemi di veicoli autonomi e/o alcuni componenti dell'infrastruttura di trasporto; tuttavia, condizioni meteorologiche, altri utenti
	della strada (guidatori non autonomi, ciclisti, pedoni, ecc.) e condizioni
	della superficie stradale non sono state incluse [31].
	\end{comment}


\subsection{Autonomus driver simulator}
Lo sviluppo della guida senza conducente è un argomento attuale e di
grande interesse, su cui molte società automobilistiche stanno investendo
dato che promette di essere un mercato da miliardi di dollari. %[15]
Di contro, la guida in ambiente cittadino è una sfida
molto più grande, per via del maggiore grado di variabilità degli scenari
che si possono presentare e degli oggetti che devono essere riconosciuti,
ed è necessario ancora molto lavoro prima che sia disponibile un sistema
in grado di muoversi da solo in questi ambienti senza pericolo e in modo
efficiente.
Nell'ottica dello sviluppo di un sistema di guida autonoma efficiente
e sicuro, si rende necessario lo sviluppo di metodi e strumenti per il
miglioramento dell'efficienza del prodotto e per la validazione dello
stesso. Tra le tecniche utilizzate per automobili a guida autonoma ci sono
metodi di apprendimento automatico che richiedono una gran quantità di
immagini, inoltre sia per lo sviluppo che soprattutto per la validazione, si
deve considerare che esistono una quantità infinita di scenari (situazioni)
che si possono presentare e che il sistema progettato deve essere in grado
di agire nel modo più sicuro in ognuno di essi. Il problema è di per sé mal
posto, visto che non è dato sapere se un'auto che reagisce correttamente
in una certa situazione farà lo stesso cambiando qualche variabile dello
scenario.
Lo sviluppo e la validazione delle auto a guida autonoma si configura
per quanto detto come molto complicato e costoso, visto che è necessario
effettuare un gran numero di prove in diversi scenari.
Per aiutare a risolvere il problema dei costi dello sviluppo con testing
in ambiente fisico e della richiesta di dati che abbiamo presentato, sono
stati sviluppati dei simulatori che permettono di sperimentare il software
sviluppato in un ambiente virtuale, tra questi NVIDIA DRIVE Constellation, AirSim, e CARLA Simulator. Questi simulatori riproducono scenari di guida cittadina
in cui è possibile testare un auto a guida autonoma che raccoglie dati
dall'ambiente. Per le considerazioni fatte in precedenza, un simulatore
si presenta come un alleato fondamentale nel processo di sviluppo e di
validazione. Infatti l'alta avidità di dati da parte degli algoritmi di machine learning impiegate dal sistema di guida autonoma, viene soddisfatto dalla capacità del sistema simulatore, di generare centinaia di migliaia di riproduzioni, nello specifico quello fisicamente e eticamente più difficili da riprodurre nello sviluppo su strada.
	\begin{comment}
	 
	Infatti un simulatore in grado di riprodurre realisticamente
	il complicato ambiente cittadino può essere utilizzato per lo sviluppo di sistemi a guida autonomi, che impiegano tecniche di machine learning avide di dati.
	Per far ciò sono necessari
	centinaia di migliaia di riproduzioni per ottenere risultati, e per testare il
	sistema in altrettanti scenari, anche eticamente o fisicamente difficili da
	riprodurre, prima o congiuntamente allo sviluppo e al testing su strada. (riguarda)
	\end{comment}
\subsubsection{CARLA}
Tra quelli citati in precedenza, il simulatore  impiegato per il progetto, è CARLA.
Un simulatore grafico open source costituito da un'architettura client-server scalabile, dove il server è responsabile  di tutto ciò che riguarda la simulazione dell'ambiente in tutti i suoi aspetti: rendering del sensore, calcolo della fisica, aggiornamenti sullo stato del mondo e sui suoi attori e molto altro. 
Mentre il lato client consiste in una somma di moduli client che controllano la logica degli attori sulla scena e impostano le condizioni del mondo.
\subsubsection{Sensori}
Come in un  veicolo reale, CARLA predispone una suite di sensori preimpostasti, fotocamere RGB e pseudo sensori che forniscono ground-truth depth e ground-truth semantic segmentation .
La loro posizione, numero e direzione possono essere specificate dal client.
%Per una presentazione completa dei sensori si rimanda a \cite{tesi niccolo galli}
E proprio della fotocamera RGB di CARLA si farà uso al fine di acquisire le immagini per il dataset.

\section{Costruzione del dataset}
<<<<<<< HEAD
Un agente che impiega il paradigma del supervised learning, richiede un dataset, per formare la propria conoscenza di base del mondo che lo circonda. Nel caso di un classificatore di immagini, la componente principale di questa mole di dati, sono le immagini. A ciascuna sarà apposta in pre-elaborazione una etichetta che identifichi la classe del dato, per poi essere suddivise in tre dataset diversi, uno per ogni fase del procedimento di training.
In maniera più schematica la creazione del dataset si basa sulla seguente procedura:
\begin{comment}
=======
Il detector al suo interno presenta
>>>>>>> a46aca1ea7e3f9e5b011cefd303503cbed294950
Il dataset è una delle componenti principali nel istruzione di un agente supervisionato, essendo il suo materiale di studio. 
Nel caso di un agente supervisionato, il dataset rappresenta la sua conoscenza di base, ovvero tutto ciò che lui sa sul mondo, e più questa conoscenza è vasta e variegata, più che l'agente è in grado di adattarsi alle situazioni che si presentano.
CARLA grazie alle sue caratteristiche di piattaforma di simulazione, ha permesso di ottenere questa grossa base di dati differenziati per costruire le fondamenta del dataset che sarà impiegato.
Prima comunque di acquisire il ddata
<<<<<<< HEAD
Il detector utilizza come tecnologia una Convolutional Neural Network, una forma di rete neurale. 
Un vantaggio dell'utilizzo di software di simulazione come CARLA, oltre a permettere un abbattimento dei costi di ricerca, permette di avere a disposizione grandi quantità di dati, utili al training dell'agente.
Infatti per l'apprendimento è stato usato il paradigma del supervised learning.
L'addestramento di una generica rete neurale prevede la creazione di un dataset sulla
base della procedura:\end{comment}

\begin{itemize}
\item Definizione delle classi
\item Acquisizione delle immagini
\item Sporcatura delle immagini
\item Suddivisione del dataset
\end{itemize}
=======
	\begin{comment}
	Il detector utilizza come tecnologia una Convolutional Neural Network, una forma di rete neurale. 
	Un vantaggio dell'utilizzo di software di simulazione come CARLA, oltre a permettere un abbattimento dei costi di ricerca, permette di avere a disposizione grandi quantità di dati, utili al training dell'agente.
	Infatti per l'apprendimento è stato usato il paradigma del supervised learning.\end{comment}
L'addestramento di una generica rete neurale prevede la creazione di un dataset sulla
base della procedura:
	\begin{itemize}
	\item Definizione delle classi
	\item Acquisizione immagini
	\item Sporcatura immagini
	\item Suddivisione del dataset
	\end{itemize}
>>>>>>> a46aca1ea7e3f9e5b011cefd303503cbed294950

Ciascuno step è illustrato in dettaglio nei paragrafi seguenti.

	\begin{comment}
		Questo prevede che siano forniti tre diverse dataset di immagini:
		\begin{itemize}
			\item Training set
			\item Validation set
			\item Test set
		\end{itemize}
	
	Per il seguente elaborato avendo la possibilità di acquisire grandi quantità di dati come verrà esposto in seguito, il training set avrà dimensioni di 240000 immagini, suddivise equamente tra pulite e sporche
	Per costruire questo, si necessità l'acquisizione  di immagini simili a  quelle ottenibili da una telecamera posto su una vettura.
	Nel caso della seguente tesi, il classificatore è  di tipo binario, quindi classifica le immagini tra pulite o sporche, quindi a sua volta il dataset sarà  suddiviso in due classi o label.  
	\end{comment}

\subsection{Definizione delle classi}
Il problema della classificazione delle immagini è un problema che come dice il verbo, deve assegnare a un elemento in input una classe.
Quindi le classi, o etichette(label) sono gruppi di elementi che hanno determinate caratteristiche in comune. Più le differenze tra classi diverse, sono poco accentuate, più è difficile distinguerle.
Nel caso del detector,  le classi rispecchiano i diversi guasti che la fotocamera può subire. Per una esposizione completa dei guasti che sono stati presi in considerazione si rimanda a \cite{secci2020failures}.
Le classi prese in considerazione per la costruzione dei vari dataset su cui sarà allenato l'agente, sono 18, di cui una rappresenta le immagini pulite(Golden Run), 16 rappresentano ciascuna un determinato guasto, e l'ultima (All) contiene al suo interno tutte e 16 i guasti presi in considerazione
\begin{multicols}{3}
\begin{itemize}
\item Golden Run
\item Blur
\item Black
\item Brightness
\item 50 death pixels
\item 200 death pixels
\item Nodemos
\item Noise
\item Sharpness
\item Brokenlen
\item Icelens
\item Banding
\item Greyscale
\item Condensation
\item Dirty lens
\item Chromatic aberration
\item Rain
\item All
\end{itemize}
\end{multicols}


\subsection{Acquisizione}
Definite le classi, la fase successiva nella costruzione dei dataset, è l'acquisizione delle immagini.
Per questa fase entra in gioco CARLA. Avviato il suo lato server, il lato client è stato impostato per generare 517 simulazioni diverse, ovvero 517 ambienti generati casualmente, con condizioni atmosferiche diverse, e in localizzate in luoghi diversi sulla stessa mappa. In ciascuna delle simulazioni il veicolo si muove nelle vie della città, seguendo percorsi randomici. Durante il tragitto il Client è stato impostato per acquisire 300 immagini dal sensore della fotocamera frontale della vettura, nel formato .png e ciascuna dalle  dimensioni di 800*600 pixels.
Si rimanda alla sezione "Manuale Utente" per le note sugli script utilizzati
\begin{comment}
Per l'acquisizione delle immagini 
Per acquisire questa mole di dati non potendo usare un sistema fisico per l'acquisizione, dati le  problematiche già  citate, è stato impiegato CARLA.
Sono state avviate 500 simulazioni, con caratteristiche diverse, come città,meteo e  elementi ambientali, generate in maniera casuale dal sistema. Da ogni run, attraverso il progetto github \textbf{cita}, è stato possibile acquisire 300 frame della fotocamera RGB posto sul veicolo, mentre questo sis spostava nell'ambiente. 
Le immagini cosi ottenute, sono del formato (800*600) nel formato  .png.
Queste rappresentano la base di partenza per la creazione del dataset, essendo tutte immagini pulite, ovvero immagini che non presentano malfunzionamenti o modifiche di qualsiasi genere.
\end{comment}
\subsection{Sporcatura}
Ottenute queste grande mole di immagini pulite, prive di qualsiasi "errore", questa fase del processo, prevede l'applicazione dei guasti alle immagini.
Si intende l'applicazione di filtri, che simulino il guasto alla fotocamera, della corrispettiva classe.
I filtri sono ripresi dal progetto github di Francesco Secci, Python Image Failures, i quali sono stati riadatti per l'uso nella seguente tesi.
\subparagraph{Rappresentazione dei vari guasti con breve descrizione}
Per una illustrazione completa dei guasti si rimanda  a \cite{secci2020failures}, da cui è stato ripreso lo studio su l'effetto di determinati guasti alla fotocamera, portino a un rate di fallimenti più elevato. 
Di seguito si riporta una breve rappresentazione della stessa immagini, acquisita dalla fotocamera di un veicolo nel simulatore CARLA, a cui è applicata i diversi filtri che simulano i guasti impiegati nel progetto.


	\begin{figure}
	     \centering
	     \begin{subfigure}[b]{0.3\textwidth}
	         \centering
	 		 \includegraphics[scale=0.1]{./foto_sporcature/original.png}
	         \caption{golden run}
	         \label{fig:golden run}
	     \end{subfigure}
	     \hfill
	     \begin{subfigure}[b]{0.3\textwidth}
	         \centering
			 \includegraphics[scale=0.1]{./foto_sporcature/50_death_pixels.png}
	         \caption{50 death pixel}
	         \label{fig:50}
	     \end{subfigure}
	     \hfill
	     \begin{subfigure}[b]{0.3\textwidth}
	         \centering
			 \includegraphics[scale=0.1]{./foto_sporcature/200_death_pixels.png}
	         \caption{200 death pixel}
	         \label{fig:200}
	     \end{subfigure}
	     \hfill
	     \begin{subfigure}[b]{0.3\textwidth}
	         \centering
		     \includegraphics[scale=0.1]{./foto_sporcature/banding.png}
	         \caption{banding}
	         \label{fig:banding}
	     \end{subfigure}
	     \hfill
	     \begin{subfigure}[b]{0.3\textwidth}
	         \centering
			 \includegraphics[scale=0.1]{./foto_sporcature/birghtness.png}
	    	 \caption{brightness}
	         \label{fig:brightness}
	     \end{subfigure}
	     \hfill
	     \begin{subfigure}[b]{0.3\textwidth}
	         \centering
			 \includegraphics[scale=0.1]{./foto_sporcature/black.png}
	         \caption{black}
	         \label{fig:black}
	     \end{subfigure}
	     \hfill
	     \begin{subfigure}[b]{0.3\textwidth}
	         \centering
			 \includegraphics[scale=0.1]{./foto_sporcature/blur.png}
	         \caption{blur 3.5}
	         \label{fig:blur}
	     \end{subfigure}
	     \hfill
	     \begin{subfigure}[b]{0.3\textwidth}
	         \centering
			 \includegraphics[scale=0.1]{./foto_sporcature/brokenlens.png}
	         \caption{brokenlens}
	         \label{fig:brokenlens}
	     \end{subfigure}
	     \hfill
	     \begin{subfigure}[b]{0.3\textwidth}
	         \centering
			 \includegraphics[scale=0.1]{./foto_sporcature/chromaticaberrration.png}
	         \caption{chromaticaberrration}
	         \label{fig:chromaticaberrration}
	     \end{subfigure}
	     \hfill
	     \begin{subfigure}[b]{0.3\textwidth}
	         \centering
			 \includegraphics[scale=0.1]{./foto_sporcature/condensation.png}
	         \caption{condensation}
	         \label{fig:condensation}
	     \end{subfigure}
	     \hfill
	     \begin{subfigure}[b]{0.3\textwidth}
	         \centering
			 \includegraphics[scale=0.1]{./foto_sporcature/dirty_lens.png}
	         \caption{dirty lens}
	         \label{fig:dirty_lens}
	     \end{subfigure}
	     \hfill
	     \begin{subfigure}[b]{0.3\textwidth}
	         \centering
			 \includegraphics[scale=0.1]{./foto_sporcature/greyscale.png}
	         \caption{greyscale}
	         \label{fig:greyscale}
	     \end{subfigure}
	     \hfill
	     \begin{subfigure}[b]{0.3\textwidth}
	         \centering
			 \includegraphics[scale=0.1]{./foto_sporcature/icelens.png}
	         \caption{icelens}
	         \label{fig:icelens}
	     \end{subfigure}
	     \hfill
	     \begin{subfigure}[b]{0.3\textwidth}
	         \centering
			 \includegraphics[scale=0.05]{./foto_sporcature/nodemos.png}
	         \caption{nodemos}
	         \label{fig:nodemos}
	     \end{subfigure}
	     \hfill
	     \begin{subfigure}[b]{0.3\textwidth}
	         \centering
			 \includegraphics[scale=0.1]{./foto_sporcature/noise.png}
	         \caption{noise}
	         \label{fig:noise}
	     \end{subfigure}
	     \hfill
	     \begin{subfigure}[b]{0.3\textwidth}
	         \centering
			 \includegraphics[scale=0.1]{./foto_sporcature/rain.png}
	         \caption{rain}
	         \label{fig:rain}
	     \end{subfigure}
	        \hfill
	     \begin{subfigure}[b]{0.3\textwidth}
	         \centering
			\includegraphics[scale=0.1]{./foto_sporcature/sharpness.png}
	         \caption{sharpness}
	         \label{fig:sharpness}
	     \end{subfigure}
	        \caption{Esempi dei filtri applicati}
	        \label{fig:Esempi dei filtri applicati}
	\end{figure}

\subsection{Suddivisione del dataset}
La Convnet sviluppata, è un classificatore binario, e come tale distingue solamente tra due classi, immagini pulite e immagini sporche, ovvero: golden run e malfunzionamento.
Detto ciò sono stati costruiti 17 dataset diversi, ciascuno per ogni classe.Ogni dataset è composto da 314400 elementi, di cui $\simeq 75\%$ come training set,       $\simeq20\%$ come validation set, e il restante  $\simeq5\%$ come test set. Ognuno di questi 3, è a sua volta equamente suddiviso nelle due classi che lo compongono.
 
Questa suddivisione tra training set e validation set delle immagini, risulta molto importante nella fase di training dei modelli, come si vedrà in seguito.
Uno schema di suddivisione (o immagine):
\begin{itemize}
\item Training set:240000 elementi
\begin{itemize}
\item Golden Run: 120000 elementi
\item Dirty Run: 120000 elementi
\end{itemize}
\item Validation set: 60000 elementi
\begin{itemize}
\item Golden Run: 30000 elementi
\item Dirty Run: 30000 elementi
\end{itemize}
\item Test set: 14400 elementi
\begin{itemize}
\item Golden Run: 7200 elementi
\item Dirty Run: 7200 elementi
\end{itemize}
\end{itemize}

\section{Costruzione del detector}
Lo scopo del detector, è quello di individuare all'interno del flusso di immagini della fotocamera, la presenza di distorsioni, ovvero riuscire a prevenire l'insorgere dei fallimenti nel sistema decisionale a causa di un guasto alla fotocamera. Per perseguire il suo scopo, il detector deve essere in grado di classificare ciascuna immagine del flusso, tra immagini pulite, ovvero immagini che non generano fallimenti nel sistema,e immagini sporche, generate da un guasto.Questa distinzione binaria, tra le due sole tipologie di immagini, scaturisce dal fatto che non è richiesta l'identificazione precisa di un singolo guasto,(multiclassificatore), ma si richiede solo la distinzione tra elemento buone e elemento cattive.
L'elemento classificatore è svolto da una rete neurale convuluzionale o ConvNet.
Come dice il nome, la  ConvNet è  una tecnologia basata sulle rete neurali più generiche, ed è già ampiamente impiegata nei sistemi di riconoscimento grafico, come classificatori, object detection oltre a vocal recognition.

\subsection{Convulutional Neural Network}
Una rete neurale convoluzionale è una rete neurale di tipo feed-forward network dove la strato convoluzionale compare almeno una volta.
L'input di una convolutional neurale network o ConvNet è una immagine, ossia una matrice di valori di ogni singolo pixel occupante una precisa posizione all'interno dell'immagine.Nel caso di immagini RGB, questa sarà descritta da un terna di matrici dell'intensità dei colori primari (rosso,verde,blu), nel caso di immagini in bianco e nero, queste sono descritte da una singola matrice. Le matrici che compongono le immagini, identificano i \textbf{canali} per la rete. %Questa con le dimensioni dell'immagine,(heigth e width) compongono l'input della rete.
\subsubsection{L'architettura} 
Le reti neurali convoluzionali operano su strutture a griglia contraddistinte da relazioni spaziali tra pixel, ereditate da uno strato al successivo tramite valori che descrivono
piccole regioni locali dello strato precedente. L’insieme di matrici degli strati nascosti (hidden layer), risultato della convoluzione o di altre operazioni, è definita feature map o activation map, i parametri addestrabili sono tensori denominati filtri o kernel.
Una CNN è composta da  un insieme di strati che si susseguono, illustrati in dettaglio nei paragrafi seguenti. Gli strati principali sono:
\begin{itemize}
\item convolutional layer;
\item activation layer;
\item pooling layer;
\item dense layer;
\end{itemize}
\cite{torresin2019sviluppo}
Di seguito la struttura sviluppata per il progetto:
\textbf{Inserisci immagine}
\begin{itemize}
	\item conv2d
	\item max pooling2d
	\item conv2d
	\item max pooling2d
	\item conv2d
	\item max pooling2d
	\item flatten
	\item dense
	\item dense
\end{itemize}

\subsubsection{Convolutional layer}

Per l'analisi delle immagini,il ConvLayer ha dimostrato di avere molto successo
nell'ottenere buoni risultati, grazie al fatto che sfrutti la caratteristica di un immagine di essere un insieme di tanti piccoli particolari. Per estrarli, viene applicata  una piccola matrice o filtro, di dimensioni
[n x n x profondità], in ogni possibile posizione dell’immagine, coprendola interamente, e computa il prodotto scalare tra il filtro stesso e la matrice corrispondente del volume di input, avente
eguale dimensioni. È possibile visualizzare la convoluzione come una sovrapposizione del kernel sull’immagine in input (o strato nascosto). \cite{aggarwal2018neural} 
Un altro esempio

 Nella Figura 4, un filtro 5x5x1,  dove nello strato rosso viene mostrato come un'area rossa più scura più piccola che ne restituisce cinque
neuroni allo strato blu. Il campo ricettivo viene quindi applicato
l'intera area rossa, emettendo neuroni in ogni punto che poi
formano lo strato blu. Quanti pixel si muove il campo ricettivo
tra ogni operazione si chiama 'stride'. Con un passo di uno, il
lo strato blu avrà approssimativamente la stessa altezza e larghezza di
lo strato rosso. Con un passo di due, sarà circa la metà.
"Approssimativamente" viene utilizzato sopra per quanto riguarda la dimensione, come
la dimensione effettiva dipende dal tipo di padding a utilizzata lungo il
bordi. "Zero padding", che significa riempire l'area intorno al
bordi con zeri, restituisce esattamente la stessa larghezza e altezza per
il livello successivo, mentre "valid-paggin risulta in entrambe le larghezze
e l'altezza è diminuita.



Un filtro è caratterizzato da un insieme di iperparametri (hyperparameters) come: altezza,larghezza,profondità e numero. 
Solitamente i filtri hanno forma quadrata e profondità uguale a quella dello strato al
quale sono applicati, nel caso di immagini RGB, 3.
 Il numero di filtri è un iperparametro definito sulla base della capacità di distinguere forme sempre più complesse che
si vuole conferire alla rete. I filtri sono dunque i componenti a cui saranno associate le
caratteristiche dei pattern delle immagini.
I filtri dei primi strati individuano forme primitive, quelli successvi imparano a distinguere forme sempre più grandi e complesse. Una proprietà della convoluzione è
l’\textbf{equivarianza alla traslazione}: immagini traslate sono interpretate allo stesso modo e
i valori delle mappa di attivazione traslano con i valori di input \cite{torresin2019sviluppo}. Ciò significa che forme particolari generano feature maps simili, indipendentemente dalla loro collocazione
nell’immagine.

Il numero di possibili allineamenti tra filtro e immagine definisce altezza e larghezza della successiva feature
map.
Dal hidden layer q1, il filtro produce uno nuovo hidden layer q2
Lq+1 = Lq - Fq + 1
Bq+1 = Bq - Fq + 1
Ad esempio, per immagini di dimensioni 32 * 32, un filtro 5 * 5 genera uno strato
nascosto 28 x 28.
È opportuno fare distinzione tra profondità del filtro e profondità di strato nascosto/mappa di attivazione: la prima, d è la stessa dello strato al quale è applicato, la
seconda deriva invece dal numero di filtri applicati.
Una delle proprietà della convoluzione è la seguente:
Una convoluzione sullo strato q incrementa il campo recettivo di una feature dallo strato q allo strato q + 1. In altre parole, ogni valore della mappa di attivazione dello strato successivo cattura una regione
spaziale più ampia del precedente. Feature maps degli strati catturano aspetti caratteristici di regioni via via maggiori e questo è il motivo per cui le CNN possono essere
definite “profonde”: per studiare l’intera immagine sono necessarie lunghe successioni
di “blocchi” di strati. 

\subparagraph{Padding} L’operazione di convoluzione comporta una contrazione dello strato q rispetto a q + 1 ed una conseguente perdita di informazioni. Il problema può essere
arginato utilizzando il cosiddetto padding, una tecnica che prevede l’aggiunta di $\frac{(Fmeno1)}{2}$ pixel ai bordi delle mappe di attivazione per mantenere l’impronta spaziale. Ovviamente, 
per non alterare l’informazione, ai pixel sono assegnati valori nulli. Il risultato
\cite{aggarwal2018neural} è un incremento delle dimensioni (altezza e larghezza) del volume di input di F - 1,
esattamente la quantità di cui è ridotto a seguito della convoluzione.
Essendo il prodotto zero, le regioni esterne soggette a padding non contribuiscono al risultato finale del prodotto scalare. Ciò che invece accade è permettere al filtro convoluzionale di scavalcare i bordi dello strato e computare il prodotto scalare
solamente per le celle di valori diversi da 0. Questa tipologia di padding è definita
“half-padding” in quanto circa metà del filtro oltrepassa i bordi, quando collocato alle
estremità. L’half-padding è utilizzato per mantenere il “footprint” spaziale.
Quando il padding non è utilizzato, si parla semplicemente di valid-padding e nella
pratica non dà buoni risultati per il seguente motivo: mentre con l’half-padding le
celle ai bordi contribuiscono all’informazione, nel caso di valid-padding, queste non
vedono il passaggio del filtro e sono sotto-rappresentate.
Un’altra forma di padding è il full-padding, con il quale si lascia che il filtro esuli
completamente dal layer andando ad occupare celle di soli zeri. Così facendo si incrementa l'impronta spaziale dello strato, allo stesso modo in cui il valid-padding la
riduce.\cite{aggarwal2018neural} 
\begin{comment}

\subparagraph{Strides} Un filtro convoluzionale computa il prodotto scalare in ogni singola posizione dello strato di input ma è altresì possibile limitare la computazione ad un numero
inferiore di posizioni, facendo uso dello stride S. La convoluzione è allora applicata
alle posizioni 1, S + 1, 2S + 1 etc., lungo entrambe le dimensioni. L’output avrà altezza
e larghezza, rispettivamente:
Lq+1 = Lq - F/S+ 1
Bq+1 = Bq - F/S+1
Ne consegue che lo stride comporta una riduzione delle dimensioni di un fattore
di circa 1/S, e dell’area di S alla seconda 2. Generalmente si usano valori limitati a 1 o 2 mentre con
stride maggiori si punta alla riduzione della richiesta di memoria. Tramite lo stride è
possibile catturare pattern complessi in vaste porzioni di immagine, con risultati simili
a quelli prodotti dal max-pooling.
In genere le dimensioni delle immagini in input sono ridotte a L = B, per evitare complicazioni nella definizione degli iperparametri. Per quanto concerne la convoluzione,
il numero di filtri è solitamente potenza di 2 per facilitare la computazione, lo stride di
1 o 2, le dimensioni del filtro di 3 o 5. Filtri piccoli significano reti più profonde e più
performanti.
Ogni filtro convoluzionale è infine associato ad un bias: dato un filtro p, ed il layer
q, bias è indicato con b(p, q). Il bias è un fattore di moltiplicazione della mappa di
attivazione e la sua presenza incrementa il numero di parametri di un’unità. Il numero
di features in ogni strato sarà dunque 1 + L x B x d. Come tutti gli altri parametri, il
valore del bias è definito tramite retropropagazione in fase di addestramento. \cite{aggarwal2018neural}\end{comment}
\subsubsection{Funzione di attivazione Relu}
L’operazione di attivazione non-lineare segue l’operazione di convoluzione. Per ogni
strato Lq x Bq x d, la funzione di attivazione genera uno strato di eguale dimensione
Lq x Bq x d di valori limitati da soglie: in quanto semplice mappatura uno-a-uno dei
valori di attivazione, la funzione ReLU non altera l'impronta spaziale dello strato.
L’attivazione avviene tramite funzioni matematiche. Mentre in passato la tangente iperbolica, la funzione sigmoide, softsign godevano di ampia diffusione, ora sono
limitate a reti non-profonde e ormai rimpiazzate dalla funzione di attivazione ReLU
(Rectified Linear Unit). Il motivo principale è che in reti neurali profonde, il gradiente
di queste tre funzioni di attivazione si annulla durante la retropropagazione ed impedisce all’algoritmo di proseguire con l’addestramento. Inoltre, la funzione ReLU è
computazionalmente molto più efficiente.
\begin{comment}
ReLU è di fatto una funzione di rimozione di valori negativi rivelatasi efficace ed efficiente: la sua derivata in IR+ è sempre 1 e non satura in IR; in altre parole, il codominio
della funzione è [0, inf). In corrispondenza dell’origine, la funzione non approssima
l’identità e genera invece un gradiente elevato impedendone la scomparsa. \cite{aghdam2017guide}
\end{comment}
\subsubsection{Pooling layer}
Il max-pooling estrae il massimo valore contenuto in matrici P x P di ogni mappa di
attivazione e produce un altro hidden layer di eguale profondità. Anche in questo caso,
come per la convoluzione, si fa uso di stride: se lo stride è 1 lo strato 2 così generato
avrà dimensioni:
L2 = L1 - P + 1
B2 = B1 - P + 1
d2 = d1.
Per stride maggiori di 1, come solitamente si usa, altezza e larghezza saranno, rispettivamente:
L2 = L1 - P/S + 1
B2 = B1 - P/S+ 1
Rispetto alla convoluzione, il pooling è effettuato al livello di ciascuna mappa di attivazione, quindi il loro numero rimane alterato e l’output è uno strato della stessa profondità (ma di altezza e larghezza differenti).

 Una configurazione tipica è dimensione
2x2 e stride 2: così facendo non c’è sovrapposizione tra regioni.
L’uso dello stride nel pooling è importante per tre motivi. Il primo è la riduzione
dell’impronta spaziale delle mappe di attivazione, il secondo è un certo grado di invarianza alla traslazione e il terzo è un incremento del campo ricettivo. Si osservi che per
ridurre l’impronta spaziale possono essere utilizzati unicamente strati convoluzionali
con stride maggiori di 1. Nonostante ciò, si preferisce tuttora utilizzare max-pooling o
qualche altra variante dato il grado di non-linearità e invarianza alla traslazione che
introducono.\cite{aggarwal2018neural}
\subsubsection{Dense layer}
Connette ogni feature in input con la corrispondente feature in output. Presenta la struttura di una rete feed-forward tradizionale e aumenta a dismisura il numero di parametri
addestrabili per effetto del numero di connessioni. Ad esempio, se due strati completamente connessi hanno 4096 unità ciascuno, il numero di connessioni (quindi di
parametri) sarà superiore a 16 milioni. \cite{aggarwal2018neural}

\subsection{La rete}
La struttura della rete sviluppata nel seguente progetto è mostrata in seguito: (temporanea)\\
\includegraphics[scale=0.5]{Struttura-della_rete1.png}
\begin{comment}
Per la  costruzione del detector,  è stata implementata una rete neurale convuluzionale. 
\begin{figure}[h!]
  \includegraphics[scale=0.1]{"cnnmodel"}
  \caption{CNN detector}
  \label{fig:CNN}
\end{figure}

Come si nota dalla figura ~\ref{fig:CNN}, questa è composta da un insieme di layer.
Il primo layer ha un è un conv2D layer, di ben 800*600*3=1440000 neuroni.
Le dimensioni di questo primo sono date dalla necessità di poter elaborare un immagine per intero, e infatti (800,600) sono le dimensioni di larghezza e altezza delle immagini, e quindi il numero di pixelche compongono l'immagine sono 800*600=480000,  l'ultimo fattore è dato dalla composizione interna di un pixel. Questo rappresenta il numero di canali dei colori,ed essendo immagini rgb (red,green,blue) sono 3.
Definito l'input del primo layer e quindi dell'intera rete, si può passare all'output.
 \end{comment}
\subsection{Addestramento del rete convuluzionale}
Dopo aver definito la struttura della rete, e avendo suddiviso il dataset in tre parti: training set, validation set e test set, la fase successiva prevede l'addestramento della rete.
Per addestramento di una rete si intendo elaborazione da parte della  rete del training set, ovvero la modifica dei pesi dei collegamenti, al fine di modellare il meglio possibile la rete sui dati. 
%il processo di determinazione dei parametri ideali che compongono un modello. google
In generale il sistema di apprendimento prevede che sia utilizzato il training set per l'addestramento della regressione logica, ovvero una modifica dei pesi 

Durante l'addestramento della CNN, il set di dati deve essere suddiviso in tre parti:
il training set, il validation set e il test set. Il training set
set viene utilizzato per addestrare la CNN utilizzando la regressione logistica, che addestrerà la rete neurale convoluzionale, che sarà spiegato di seguito. Dopo ogni round di training, la rete è
testata su una parte casuale del validation set. Il risultato viene utilizzato per valutare l'accuratezza della rete nel rilevare gli oggetti corretti,
al fine di determinare se i pesi vengono regolati nel modo migliore.
Il test set viene utilizzato per valutare le prestazioni della CNN dopo che
il processo di formazione è terminato. Tuttavia, può essere pericoloso modificare
la rete in base ai risultati utilizzando il set di test, a causa del rischio
di overfitting. La dimensione esatta dei diversi set varia a seconda
sul set di dati fornito, ma di solito il set di addestramento è composto da
circa il 75-80\% dei dati totali, mentre la convalida e
i set di test utilizzano entrambi metà dei dati rimanenti, circa 10-
12,5\% ciascuno \cite{guyon1997scaling}.

Avendo predisposto la rete, e i vari dataset, il passo successivo è il suo addestramento.

\subsubsection{Batch}
Siccome le reti richiedono grosse quantità di dati per essere allenate, si definiscono due modi per fare ciò: la prima prevede l'uso di una infrastruttura hardware in grado di processare l'intera mole tutta insieme nello stesso momento, e questo può risultare effettivamente difficile, mentre il secondo modo prevede di dividere il dataset in porzioni, e fornirle una per una alla rete.
Le dimensioni di queste porzioni, è definità come \textbf{Batch Size}.
Il numero di batch  necessari a processare l'interno dataset, definisce il numero di iterazioni.

\subsubsection{Epochs}
Ultima componente fondamentale di un addestramento, sono le Epochs.
Dato il dataset di 300000 elementi, e un batch size di 4, si avranno 75000 iterazioni per epoch, quindi un epoch è l'intero ciclo di elaborazione di un training set, e le epochs identificano quante epoch saranno eseguite. Questo elemento è importante, perchè ad ogni epoch mantenendo il dataset costante, modifica i pesi della rete, spostando la curva di apprendimento tra uno stato underfitting, a uno ottimale, a un overfitting. 
Il numero di epoch necessario a ottenere una curva ottimale varia da dataset a dataset

\subsection{Testing della rete}
Ottenuto 17 reti istruite ciascuna sul proprio dataset di dati, la fase finale del progetto è verificare la loro effettiva efficacia su un set di immagini mai utilizzato. Questo per verificare che la rete non abbia fatto un overfitting sui dati, e quindi la sua  generalità nell'applicazione.
Per ciascuna rete sono stati impiegati due differenti test set. Uno composto da golden run-singolo guasto e l'altro golden run-multiguasto. 
Si è usato la confusion matrix
\section{Esecuzione e risultati}

\section{Conclusioni e lavori futuri}

\section{A Manuale utente}

\newpage
\printbibliography

\end{document}
